{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixSi6udtrYZk"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/02-langsmith.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NwdBTO0qCQi"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swum099VqCQj"
      },
      "source": [
        "# LangSmith Starter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hk437ZjqCQj"
      },
      "source": [
        "LangSmith is a built-in observability service and platform that integrates _very easily_ with LangChain. You don't _need_ to use LangSmith for this course, but it can be very helpful in understanding what is happening, _and_ we recommend using it beyond this course for general development with LangChain — with all of that in mind we would recommend spending a little bit of time to get familiar with LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k9EfPofqG3g",
        "outputId": "71873d67-b511-40b8-9524-d1b0e0ddd3e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2mResolved \u001b[1m126 packages\u001b[0m \u001b[2min 1.00s\u001b[0m\u001b[0m\n",
            "\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 914ms\u001b[0m\u001b[0m\n",
            "\u001b[2mInstalled \u001b[1m7 packages\u001b[0m \u001b[2min 400ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==0.3.23\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.88.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv add langchain-core langchain-openai langchain-community langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# I will be using Gemini for this and all the examples going ahead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UdyCjmqqCQk"
      },
      "source": [
        "## Setting up LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqURpTWbqCQk"
      },
      "source": [
        "LangSmith does require an API key, but it comes with a generous free tier. You can sign up an account and get your API key [here](https://smith.langchain.com).\n",
        "\n",
        "When using LangSmith, we need to setup our environment variables _and_ provide our API key, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezoLeIGrqCQk",
        "outputId": "252aa252-d8c3-4cd5-fb18-acf3cad2dee5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"abhimvp-langchain-deep-dive-langsmith-gemini\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGDntgWqCQk"
      },
      "source": [
        "In most cases, this is all we need to start seeing logs and traces in the [LangSmith UI](https://smith.langchain.com). By default, LangChain will trace LLM calls, chains, etc. We'll take a look at a quick example of this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFDeEk0tqCQk"
      },
      "source": [
        "## Default Tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3hqGO2qCQk"
      },
      "source": [
        "As mentioned, LangSmith traces a lot of data without us needing to do anything. Let's see how that looks. We'll start by initializing our LLM. Again, this will need an [API key](https://platform.openai.com/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AfToUsEqCQl",
        "outputId": "4426952f-af46-440d-d275-a86610a9c1fe"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVDqCttqCQl"
      },
      "source": [
        "Let's invoke our LLM and then see what happens in the LangSmith UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCGtll-IqCQl",
        "outputId": "0f9ea347-b50f-481c-c237-fd1fd6a09589"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--b61770c3-122b-49a4-ba00-cfbf2c2785d5-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYM-pIIaqCQl"
      },
      "source": [
        "After this we should see that a new project (`aurelioai-langchain-course-langsmith-openai`) has been created in the LangSmith UI. Inside that project, we should see the trace from our LLM call:\n",
        "\n",
        "\n",
        "![LangSmith LLM Trace](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-01.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7VktC46qCQl"
      },
      "source": [
        "By default, LangSmith will capture plenty — however, it won't capture functions from outside of LangChain. Let's see how we can trace those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNQRKbp9qCQl"
      },
      "source": [
        "## Tracing Non-LangChain Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbYn3yY7qCQl"
      },
      "source": [
        "LangSmith can trace functions that are not part of LangChain, we just need to add the `@traceable` decorator. Let's try this for a few simple functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NVo50uq6qCQl"
      },
      "outputs": [],
      "source": [
        "from langsmith import traceable\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "@traceable\n",
        "def generate_random_number():\n",
        "    return random.randint(0, 100)\n",
        "\n",
        "@traceable\n",
        "def generate_string_delay(input_str: str):\n",
        "    number = random.randint(1, 5)\n",
        "    time.sleep(number)\n",
        "    return f\"{input_str} ({number})\"\n",
        "\n",
        "@traceable\n",
        "def random_error():\n",
        "    number = random.randint(0, 1)\n",
        "    if number == 0:\n",
        "        raise ValueError(\"Random error\")\n",
        "    else:\n",
        "        return \"No error\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4b1yJtqCQl"
      },
      "source": [
        "Let's run these a few times and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2af699bd631846bdb2e064981a48ae67",
            "1507edeb2bcf43238ed2431731b51f24",
            "76c4884af2ed4474a07a6f8964cd7b10",
            "302271efa18349fb8fd20e2fa922f17f",
            "59d44325b22241a0828dd1df4c87eb46",
            "fb6ca9c9ad144bc3947e454cc3f3ab77",
            "1e91ab27a61c4530ac1dd2e05a4455ae",
            "a087fea20eb040469312b40081517d67",
            "ba1df7e62b114c6d9062c65a04bb349d",
            "e7967f66202744d1b41ec3ae02a45617",
            "fe1edcf8e1fa4ae2bdae23d2235395e0"
          ]
        },
        "id": "iTDAR7b4qCQl",
        "outputId": "084ca65c-2497-41cf-e6f6-5b946163b5f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abhis\\Desktop\\LangChain-v0.3-DeepDive\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "100%|██████████| 10/10 [00:28<00:00,  2.81s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for _ in tqdm(range(10)):\n",
        "    generate_random_number()\n",
        "    generate_string_delay(\"Hello\")\n",
        "    try:\n",
        "        random_error()\n",
        "    except ValueError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCoeNfRiqCQl"
      },
      "source": [
        "Those traces should now be visible in the LangSmith UI, again under the same project:\n",
        "\n",
        "![LangSmith Traces](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-02.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuze3LTqCQl"
      },
      "source": [
        "We can various metrics here for each run. First, ofcourse, the run name. We can see any inputs and outputs from each run, we can see if the run raised any errors, it's start time, and latency. Inside the UI we can also filter for specific runs, like so:\n",
        "\n",
        "![LangSmith UI filters](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-03.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7UR0DPwqCQl"
      },
      "source": [
        "There are various other things we can do within the UI, but we'll leave that for you to explore.\n",
        "\n",
        "Finally, we can also modify our traceable names if we'd like to make them more readable inside the UI. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dQLkwo0PqCQl"
      },
      "outputs": [],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "@traceable(name=\"Chitchat Maker\")\n",
        "def error_generation_function(question: str):\n",
        "    delay = random.randint(0, 3)\n",
        "    time.sleep(delay)\n",
        "    number = random.randint(0, 1)\n",
        "    if number == 0:\n",
        "        raise ValueError(\"Random error\")\n",
        "    else:\n",
        "        return \"I'm great how are you?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8NFR6qqCQl"
      },
      "source": [
        "Let's run this a few times and see what we get in LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e988534598c54261a671bc9e17fdb81b",
            "4c3b5008fb254259bfded2cffaa43790",
            "5e63efa23bd54da5bc94a6776f11d0e7",
            "15c2401e19a34ce48286460dad8bf4b2",
            "c884ebe0266b47aa9620600ed2169c81",
            "a27c7c05a70041fcb8dfab45692ef7fc",
            "c1bc2c068ee840d38a4b615f108e4e0c",
            "ffce1b8c4b8646c2b00f81058cb6e6c0",
            "786d6b07a8104f5fa499fbf345bc831f",
            "479c419c88a3443b8c39767fb3540c8b",
            "290f779d73ca42c9bd826518d686bbea"
          ]
        },
        "id": "ay8sxvXWqCQm",
        "outputId": "bafa13da-6ffe-48cb-fa5c-4e18e65a6e3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n"
          ]
        }
      ],
      "source": [
        "for _ in tqdm(range(2)):\n",
        "    try:\n",
        "        error_generation_function(\"How are you today?\")\n",
        "    except ValueError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsITHuXBqCQm"
      },
      "source": [
        "Let's filter for the `Chitchat Maker` traceable and see our results.\n",
        "\n",
        "![LangSmith Chitchat Maker runs](https://github.com/aurelio-labs/langchain-course/blob/main/assets/langsmith-ui-04.jpg?raw=1)\n",
        "\n",
        "We can see our runs and their related metadata! That's it for this intro to LangSmith. As we work through the course we will (optionally) refer to LangSmith for digging into our runs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LangChain-v0.3-DeepDive",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
